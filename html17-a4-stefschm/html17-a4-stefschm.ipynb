{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "inNotebook = True  # change this to True if you use a notebook\n",
    "def nextplot():\n",
    "    if inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()     # and this clears the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "%run -i \"neural-networks-helper.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X1 (separable)\n",
    "nextplot()\n",
    "plot3(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X2 (not separable)\n",
    "nextplot()\n",
    "plot3(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_classify(X, w):\n",
    "    \"\"\"Classify using a perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (N,D) or shape (D,)\n",
    "        Design matrix of test examples\n",
    "    w : ndarray of shape (D,)\n",
    "        Weight vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray of shape (N,)\n",
    "        Predicted binary labels (either 0 or 1)\"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape((1, X.shape[0]))\n",
    "    return (X@w >= 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a+c Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_train(X, y, maxepochs=100, pocket=False, w0=None):\n",
    "    \"\"\"Train a perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix\n",
    "    y : ndarray of shape (N,)\n",
    "        Binary labels (either 0 or 1)\n",
    "    maxepochs : int\n",
    "        Maximum number of passes through the training set before the algorithm\n",
    "        returns\n",
    "    pocket : bool\n",
    "       Whether to use the pocket algorithm (True) or the perceptron learning algorithm\n",
    "       (False)\n",
    "    w0 : ndarray of shape (D,)\n",
    "        Initial weight vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray of shape (D,)\n",
    "        Fitted weight vector\"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "    if w0 is None:        # initial weight vector\n",
    "        w0 = np.zeros(D)\n",
    "    w = w0                # current weight vector\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    ytil = list(map(lambda i: i if i == 1 else -1, y))\n",
    "    ypred = np.sign(X @ w)\n",
    "    updates = 0\n",
    "    \n",
    "    # Abort early if there are no errors or no epochs left\n",
    "    if np.array_equal(ypred, ytil) or maxepochs == 0:\n",
    "        return w\n",
    "    \n",
    "    if not pocket:\n",
    "        for i in range(N):\n",
    "            if ytil[i] != ypred[i]:\n",
    "                updates += 1\n",
    "                w += ytil[i] * X[i]\n",
    "    else:\n",
    "        curr = longest = 0\n",
    "        best_w = w\n",
    "        for _ in range(N): # Choose n random samples\n",
    "            si = np.random.choice(N, replace=True)\n",
    "            if ytil[si] == ypred[si]:\n",
    "                curr += 1\n",
    "                if curr > longest:\n",
    "                    longest = curr\n",
    "                    best_w = w\n",
    "            else:\n",
    "                updates += 1\n",
    "                w += ytil[si] * X[si]\n",
    "                curr = 0\n",
    "        w = best_w\n",
    "        \n",
    "    misclassified = len(list(filter(lambda x: x == 0, ytil + np.sign(X @ w))))\n",
    "    print(\"{} out of {} examples are misclassified and w was updated {} times\".format(misclassified, N, updates))\n",
    "    return pt_train(X, y, maxepochs - 1, pocket, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b+d Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron using the perceptron learning algorithm and plot decision\n",
    "# boundary. You should get a perfect classification here. The decision boundary\n",
    "# should not change if you run this multiple times.\n",
    "w = pt_train(X1, y1)\n",
    "nextplot()\n",
    "plot3(X1,y1)\n",
    "plot3db(w, label=\"perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a perceptron using the pocket algorithm and plot decision boundary. You\n",
    "# should get a perfect classification here (with high probability). The decision\n",
    "# boundary should change if you run this multiple times.\n",
    "w = pt_train(X1, y1, pocket=True)\n",
    "nextplot()\n",
    "plot3(X1,y1)\n",
    "plot3db(w, label=\"pocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train a perceptron using the pocket algorithm and plot decision boundary.\n",
    "w = pt_train(X2, y2, pocket=True)\n",
    "nextplot()\n",
    "plot3(X2,y2)\n",
    "plot3db(w, label=\"pocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train 10 perceptrons starting with random weights. Also train logistic\n",
    "# regression and SVM. Plot all decision boundaries, and print the\n",
    "# misclassification rates (on training data). Try this with and without the\n",
    "# pocket algorithm.\n",
    "nextplot()\n",
    "plot3dbs(X2, y2, n=10, maxepochs=1000, pocket=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi-Layer Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a Conjecture how an FNN fit will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the one-dimensional dataset that we will use\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Train with 2 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an FNN using scikit-learn. nhidden is an integer, it refers to the\n",
    "# number of units in the hidden layer.\n",
    "def train3(nhidden, activation='logistic'):\n",
    "    print(\"Training FNN with\", nhidden, \"hidden units...\")\n",
    "    if type(nhidden) is int:\n",
    "        nhidden = (nhidden,)\n",
    "    model = MLPRegressor(activation=activation,\n",
    "                         hidden_layer_sizes=nhidden,\n",
    "                         solver='lbfgs',  # better solver for small datasets\n",
    "                         alpha=0,         # no regularization\n",
    "                         max_iter=100000, tol=1e-10)\n",
    "    model.fit(X3,y3.reshape((-1,)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the model with 2 units in the hidden layer\n",
    "model = train3(2)\n",
    "print(\"Training error:\", mean_squared_error(y3, model.predict(X3)))\n",
    "print(\"Test error    :\", mean_squared_error(y3test, model.predict(X3test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and the fit\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight matrices and bias vectors can be read out as follows. If you want,\n",
    "# use these parameters to compute the output of the network (on X3) directly and\n",
    "# compare to model.predict(X3)\n",
    "weights = model.coefs_      # list of weight matrices (layer by layer)\n",
    "biases = model.intercepts_  # list of bias vectors (layer by layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now repeat this multiple times\n",
    "# YOUR CODE HERE\n",
    "for _ in range(5):\n",
    "    # Let's fit the model with 2 units in the hidden layer\n",
    "    model = train3(2)\n",
    "    print(\"Training error:\", mean_squared_error(y3, model.predict(X3)))\n",
    "    print(\"Test error    :\", mean_squared_error(y3test, model.predict(X3test)))\n",
    "    \n",
    "    # plot the data and the fit\n",
    "    nextplot()\n",
    "    plot1(X3, y3, label=\"train\")\n",
    "    plot1(X3test, y3test, label=\"test\")\n",
    "    plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c Improved training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a meta-learning algorithm that trains multiple times and returns the\n",
    "# best fit (on training data). Use train3 in your implementation and pass\n",
    "# argument \"activation\" to it as is.\n",
    "def train3improved(nhidden, repetitions=10, activation='logistic'):\n",
    "    # YOUR CODE HERE\n",
    "    best_train = 1.0\n",
    "    best_model = None\n",
    "    for i in range(repetitions):\n",
    "        model = train3(nhidden, activation=activation)\n",
    "        if mean_squared_error(y3, model.predict(X3)) < best_train:\n",
    "            best_train = mean_squared_error(y3, model.predict(X3))\n",
    "            best_model = model\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From now on, only use your improved method. Experiment with different hidden\n",
    "# layer sizes\n",
    "# YOUR CODE HERE\n",
    "hidden_units = [1, 2, 3, 10, 50, 100]\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "def plotErr(dist, errors, label='default', linestyle='-'):\n",
    "    length = range(len(dist))\n",
    "    plt.plot(length, errors, label=label, linestyle=linestyle)\n",
    "    plt.xticks(length, dist)\n",
    "    plt.xlabel('Hidden Units')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend(ncol=2)\n",
    "\n",
    "for nhidden in hidden_units:\n",
    "    model = train3improved(nhidden)\n",
    "    train_errors.append(mean_squared_error(y3, model.predict(X3)))\n",
    "    test_errors.append(mean_squared_error(y3test, model.predict(X3test)))\n",
    "    \n",
    "    # plot the data and the fit\n",
    "    nextplot()\n",
    "    plot1(X3, y3, label=\"train\")\n",
    "    plot1(X3test, y3test, label=\"test\")\n",
    "    plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model)\n",
    "\n",
    "nextplot()\n",
    "plotErr(hidden_units, train_errors, label=\"train\")\n",
    "plotErr(hidden_units, test_errors, label=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e Distributed representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do everything in one cell\n",
    "hidden_units = [2, 3, 10]\n",
    "for nhidden in hidden_units:\n",
    "    # train a model to analyze\n",
    "    model = train3improved(nhidden)\n",
    "    \n",
    "    # plot the fit as well as the outputs of each neuron in the hidden\n",
    "    # layer (scale for the latter is shown on right y-axis)\n",
    "    nextplot()\n",
    "    plot1(X3, y3, label=\"train\")\n",
    "    plot1(X3test, y3test, label=\"test\")\n",
    "    plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model, hidden=True, scale=False)\n",
    "    \n",
    "    # plot the fit as well as the outputs of each neuron in the hidden layer, scaled\n",
    "    # by its weight for the output neuron (scale for the latter is shown on right\n",
    "    # y-axis)\n",
    "    nextplot()\n",
    "    plot1(X3, y3, label=\"train\")\n",
    "    plot1(X3test, y3test, label=\"test\")\n",
    "    plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model, hidden=True, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2f Depth (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Experiment with different number of layers.\n",
    "#\n",
    "# The argument nhidden is a tuple with as many entries as there are hidden\n",
    "# layers; each entry is the size of the layer. The example code below uses two\n",
    "# hidden layers; the first hidden layer has 9 neurons, then second one has 10\n",
    "# neurons.\n",
    "#\n",
    "# The argument activation specifies the type of neurons in the hidden layers.\n",
    "# Try 'logistic', 'relu', and 'tanh'. For 'relu' units and 1 hidden layer, you\n",
    "# can also plot the outputs of the hidden neurons (using the same code above).\n",
    "model = train3improved((9,10,), 50, activation='relu')\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model)\n",
    "print(\"Training error:\", mean_squared_error(y3, model.predict(X3)))\n",
    "print(\"Test error    :\", mean_squared_error(y3test, model.predict(X3test)))\n",
    "\n",
    "model = train3improved((9,10,), 50, activation='tanh')\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model)\n",
    "print(\"Training error:\", mean_squared_error(y3, model.predict(X3)))\n",
    "print(\"Test error    :\", mean_squared_error(y3test, model.predict(X3test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2g Keras (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an FNN with one hidden layer and nhidden units using Keras\n",
    "def train3keras(nhidden_or_model, epochs=2000, lr=None, verbose=True):\n",
    "    # we force computation on the CPU (much faster for such a small model)\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        if type(nhidden_or_model) is int:\n",
    "            # our FNN is a sequence of layers\n",
    "            nhidden = nhidden_or_model\n",
    "            model = Sequential()\n",
    "\n",
    "            if nhidden > 0:\n",
    "                # create the hidden layer (fully connected, logistic units).\n",
    "                # Here we \"help\" the fitting method by provided a range of\n",
    "                # useful (random) choice for the initial biases.\n",
    "                model.add(Dense(nhidden, activation='sigmoid', input_dim=1,\n",
    "                                bias_initializer=keras.initializers.RandomUniform(0,4*np.pi)))\n",
    "\n",
    "                # create the output layer (fully connected, linear unit)\n",
    "                model.add(Dense(1, activation='linear'))\n",
    "            else:\n",
    "                # create just the output layer (fully connected, linear unit)\n",
    "                model.add(Dense(1, activation='linear', input_dim=1))\n",
    "\n",
    "            # compile the model\n",
    "            if lr is None:\n",
    "                lr = 0.01\n",
    "            model.compile(loss='mean_squared_error',                # optimize the mean squared error (MSE)\n",
    "                          optimizer=keras.optimizers.Adam(lr=lr))   # use the Adam optimizer\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Training FNN with {:d} hidden units for {:d} epochs...\".format(nhidden, epochs))\n",
    "        else:\n",
    "            model = nhidden_or_model\n",
    "            if lr is not None:\n",
    "                backend.set_value(model.optimizer.lr, lr)\n",
    "            if verbose:\n",
    "                print(\"Continuing training for another {:d} epochs...\".format(epochs))\n",
    "\n",
    "        # train the model\n",
    "        history = model.fit(X3, y3, epochs=epochs, verbose=0)\n",
    "\n",
    "        # print errors\n",
    "        mse_train = model.evaluate(X3,y3, verbose=0)\n",
    "        if verbose:\n",
    "            print(\"Training error:\", mse_train)\n",
    "\n",
    "        # return result\n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the model with 2 units in the hidden layer\n",
    "model, history = train3keras(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the progress training made over time\n",
    "nextplot()\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.ylim([0,0.5])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean squared error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model does not \"look\" converged, you may want to try more\n",
    "# epochs. You can resume training as follows:\n",
    "train3keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data and the fit\n",
    "nextplot()\n",
    "plot1(X3, y3, label=\"train\")\n",
    "plot1(X3test, y3test, label=\"test\")\n",
    "plot1fit(np.linspace(0, 13, 500)[:,np.newaxis], model, hidden=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
